{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('udacity-dend')\n",
    "for obj in bucket.objects.all():\n",
    "    key = obj.key\n",
    "    body =obj.get()['Body'].read\n",
    "    print(key, body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how we look into a single file\n",
    "\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('udacity-dend')\n",
    "for obj in bucket.objects.filter(Prefix='song_data/A/A/A/TRAAAAK128F9318786.json'):\n",
    "    print(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "dl_session = boto3.Session(profile_name='DLADMIN')\n",
    "\n",
    "s3 = dl_session.resource('s3')\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto3 import client\n",
    "\n",
    "bucket = 'udacity-dend'\n",
    "bucket2 = 's3a://udacity-dend/song_data/A/A/B/TRAABJL12903CDCF1A.json'\n",
    "file_key = 'A/A/B/TRAABJL12903CDCF1A.json'\n",
    "s3_client = client('s3', os.environ['AWS_ACCESS_KEY_ID'], os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "\n",
    "res = s3_client.list_objects_v2(Bucket=bucket)\n",
    "\n",
    "print(res)\n",
    "#if 'Contents' in res:\n",
    "#    print('Exists')\n",
    "#else:\n",
    "#    print('key DNE in bucket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "\n",
    "bucket = 'udacity-dend'\n",
    "\n",
    "file_key = 'song_data'\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "    aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY']\n",
    ")\n",
    "\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "try:\n",
    "    s3.Object(bucket, file_key).load()\n",
    "\n",
    "except botocore.exceptions.ClientError as e:\n",
    "\n",
    "    if e.response['Error']['Code'] == \"404\":\n",
    "\n",
    "        print(\"Object Doesn't exists\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(\"Error occurred while fetching a file from S3. Try Again.\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Object Exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this probably needs to be run from inside the EMR cluster unless we run spark locally\n",
    "# running it locally would be impossible for me without a VM to properly install spark on (need linux/mac)\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "song_data = \"s3a://udacity-dend/song_data/A/A/A/TRAAAAK128F9318786.json\"\n",
    "\n",
    "df = spark.read.json(song_data)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "song_data = \"s3a://udacity-dend/song_data/A/A/A/\"\n",
    "\n",
    "df = spark.read.json(song_data)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "bucket = 'udacity-dend'\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "    aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY']\n",
    ")\n",
    "\n",
    "s3 = session.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import n\n",
    "\n",
    "types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 3, 4, 5, 7, 7, 7, 9]\n",
      "4.5\n"
     ]
    }
   ],
   "source": [
    "def mergeSort(dataset):\n",
    "    if len(dataset) > 1:\n",
    "        mid = len(dataset) // 2\n",
    "        leftarr = dataset[:mid]\n",
    "        rightarr = dataset[mid:]\n",
    "\n",
    "        mergeSort(leftarr)\n",
    "        mergeSort(rightarr)\n",
    "\n",
    "        # merge:\n",
    "        leftindex = 0\n",
    "        rightindex = 0\n",
    "        mergeindex = 0\n",
    "\n",
    "        while leftindex < len(leftarr) and rightindex < len(rightarr):\n",
    "            if leftarr[leftindex] < rightarr[rightindex]:\n",
    "                dataset[mergeindex] = leftarr[leftindex]\n",
    "                leftindex += 1\n",
    "            else:\n",
    "                dataset[mergeindex] = rightarr[rightindex]\n",
    "                rightindex += 1\n",
    "            mergeindex += 1\n",
    "\n",
    "        while leftindex < len(leftarr):\n",
    "            dataset[mergeindex] = leftarr[leftindex]\n",
    "            leftindex += 1\n",
    "            mergeindex += 1\n",
    "\n",
    "        while rightindex < len(rightarr):\n",
    "            dataset[mergeindex] = rightarr[rightindex]\n",
    "            rightindex += 1\n",
    "            mergeindex += 1\n",
    "\n",
    "\n",
    "def getMedian(arr):\n",
    "    if not arr:\n",
    "        return 0\n",
    "    if (len(arr) % 2) == 0:\n",
    "        #even set, average numbers at 2 middle indices\n",
    "        i1 = len(arr)//2\n",
    "        i2 = (len(arr)//2) - 1\n",
    "        return float(arr[i1] + arr[i2])/2\n",
    "        #even set, average numbers at 2 middle indices\n",
    "    else:\n",
    "        # odd set, return number at middle index\n",
    "        return arr[len(arr)//2]\n",
    "\n",
    "arr = [9, 7, 3, 7, 2, 7, 4, 3, 5, 1]\n",
    "mergeSort(arr)\n",
    "print(arr)\n",
    "print(getMedian(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# factorial\n",
    "\n",
    "def factorial(num):\n",
    "    if num < 0:\n",
    "        raise ValueError('Factorial not possible on negative numbers.')\n",
    "    #sign = -1 if num < 0 else 1\n",
    "    if abs(num) > 1:\n",
    "        return num * factorial(num - 1)\n",
    "    else:\n",
    "        return num\n",
    "\n",
    "print(factorial(3))\n",
    "print(factorial(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_parens(s):\n",
    "    # find open parens\n",
    "    # find closed parens\n",
    "    # if no parens, return true\n",
    "    # if first parens is closed, return false\n",
    "    # if unequal number of parens, return false\n",
    "    # for character in string, make sure open and closed parens alternate\n",
    "    # could create an ordered list of indices for open and closed and make sure \n",
    "    # at a given index, closed is always greater than open\n",
    "    # need to do this for curly braces, brackets, then parens\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f340908d974e6e642b0f24874110ca1fcf85e3e8e95ea0f11995d54a92a2f68"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('data_engineering')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
